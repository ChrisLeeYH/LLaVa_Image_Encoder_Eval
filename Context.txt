=======LLM task=======

Context: Social scientists have increasingly used LLMs for content analysis. However, most studies have focused on text data. You will assess the performance of one open-sourced LLM (e.g., LLaVA) in analyzing a dataset about climate change images (https://www.frontiersin.org/journals/psychology/articles/10.3389/fpsyg.2019.00960/full). You will extract three key attributes from each image in the dataset:

1.Valence

2.Arousal

3.Relevance (how related the image is to climate change)

 

Your task involves:

1.Designing prompts to measure these attributes (they should be similar to those used in the original study).

2.Comparing LLM-generated scores against human perceptions. Calculating evaluation metrics appropriate for this task.

3.Evaluating discrepancies between LLM and humans. Come up with some possible reasons for misalignment. 

 

Deliverables:

1.A work report detailing your methods, findings, and additional observations you find interesting.

2.Python scripts used in the analysis.

3.Data generated in the study.

Addtional Analysis:
1. Do you have the correlation, MAE, and MSE between human ratings and LLM-generated ratings? I’d like to see this computed for different scenarios:

Human against one LLM rating

...The average of two LLM ratings

...The average of three LLM ratings

… all the way up to ten LLM ratings

 

2.LLaVA's Stability: You mentioned that LLaVA’s results are not stable. Have you looked into whether it has settings to control for randomness (e.g., temperature or other parameters)?

 

3.Have you examined cases where LLaVA’s ratings diverge from human ratings? For example, cases where LLaVA assigns a high relevance rating, but human evaluators give a low score. Can you provide some specific examples and possible reasons for each image attribute where such discrepancies occur?